---
title: "Regression"
author: "Claire, Hilary, Sarah, Phebe"
date: "2/15/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Load in packages 
```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels) 
tidymodels_prefer()
```


#Load in the datasets
```{r}
NHL.test <- read.csv("test.csv")

NHL.train <- read.csv("train.csv")
```

#Select 14 variables to use in the regression model
```{r}
NHL.regression <- NHL.train %>%
  select(Salary, Ht, Wt, Hand, DftRd, G, A1, DftYr, dzFOL, Cntry, GP, Position, SA)

#MGL, OpFOW not found in this dataset 
#MGL = Games lost due to injury 
#OpFOW = Opening faceoffs won
```

#Data cleaning 
```{r}
#DO WE NEED TO CLEAN THE DATA
#ideally make the birth year one whole variable instead of a bunch of yes or no (born variables)

```

#Creation of CV folds 
```{r}
set.seed(123)

NHL.cv6 <- vfold_cv(NHL.regression, v=6)
```

#Model spec
```{r}
ols.spec <-
    linear_reg() %>% 
    set_engine(engine = 'lm') %>% 
    set_mode('regression')

lm.recipe <- recipe(Salary ~ ., data = NHL.regression) %>%
    step_nzv(all_predictors()) %>% # removes variables with the same value
 #   step_corr() %>%
    step_normalize(all_numeric_predictors()) %>% # important standardization step for LASSO
    step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables

lm.workflow <- workflow() %>%
    add_recipe(lm.recipe) %>%
    add_model(ols.spec)
    
full_model <- fit(lm.workflow, data = NHL.regression) 

full_model %>% tidy()
```

#Calculate and collect CV metrics
```{r}

#NEED TO COMPARE MULTIPLE MODELS TO EACH OTHER

mod1.cv <- fit_resamples(lm.workflow,
  resamples = NHL.cv6, 
  metrics = metric_set(mae, rmse)#SPECIFY HERE?
) %>%

  collect_metrics(summarize=TRUE)

mod1.cv
```

#LASSO
```{r}
# Lasso Model Spec with tune
lasso.spec <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
  set_engine(engine = 'glmnet') %>% #note we are using a different engine
  set_mode('regression') 

# Workflow (Recipe + Model)
lasso_wf_tune <- workflow() %>% 
  add_recipe(lm.recipe) %>% # recipe defined above
  add_model(lasso.spec) 

# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(-3, 1)), #log10 transformed
  levels = 30)


tune_output <- tune_grid( # new function for tuning parameters
  lasso_wf_tune, # workflow
  resamples = NHL.cv6, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)


autoplot(tune_output)

collect_metrics(tune_output) %>%
  filter(.metric == 'rmse') %>%
  select(penalty, rmse = mean) 


best_penalty <- select_best(tune_output, metric = 'rmse') # choose best penalty value

Credit_final_wk <- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow

Credit_final_fit <- fit(Credit_final_wk, data = NHL.regression)

tidy(Credit_final_fit)

```

#Fit and tune models 
```{r}

```

#Visualize redisuals 
```{r}
#Evaluate whether some quantitative predictors might be better modeled with nonlinear relationships
```

Which variables are most important predictors of your quantitative outcome? Justify your answer. Do the methods you've applied reach consensus on which variables are most important? What insights are expected? Surprising? NOTE: if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected. 

>


Best overall model based on investigations so far? Predictive accuracy? Interpretability? A combination of both?

> 


Are there any harms that may come from your analyses and/or how the data were collected? What cautions do you want to keep in mind when communicating your work? 

>




