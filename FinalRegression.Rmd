---
title: "Regression"
author: "Claire McHenry, Hilary Kaufman, Sarah Tannert-Lerner, Phebe Chen"
date: "2/15/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning=FALSE)
```

#Load in packages 
```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels) 
tidymodels_prefer()
library(rpart.plot)
```


#Load in the datasets
```{r}
#Testing data
NHL.test <- read.csv("test.csv")

#Training data
NHL.train <- read.csv("train.csv")
```

#Select 14 variables to use in the regression model
```{r}
#Clean the data so that there are 14 variables we are looking at 
NHL.regression <- NHL.train %>%
  select(Salary, Ht, Wt, Hand, DftRd, G, A1, DftYr, dzFOL, Cntry, GP,Position,SA)

#MGL, OpFOW not found in this dataset 
#MGL = Games lost due to injury 
#OpFOW = Opening faceoffs won
```

#Data cleaning 
```{r}
#Transform the data so that it's as.numeric for Country 
#ideally make the birth year one whole variable instead of a bunch of yes or no (born variables)
NHL.regression2 <- NHL.regression %>%  
  transform(Cntry,Country=as.numeric(factor(Cntry))) %>% mutate(rookie=ifelse(NHL.regression$DftYr>2013,1,0))%>%
  select(Salary, Ht, Wt, Hand, DftRd, G, A1, DftYr, dzFOL, Country, GP, Position, SA,rookie)
NHL.regression2 <- drop_na(NHL.regression2)
```

#Creation of CV folds 
```{r}
set.seed(123)
# 6 fold cross validation
NHL.cv6 <- vfold_cv(NHL.regression2, v=6)
```

#Model spec
```{r}
# model specification for OLS
ols.spec <-
    linear_reg() %>% 
    set_engine(engine = 'lm') %>% 
    set_mode('regression')

# model recipe 
lm.recipe <- recipe(Salary ~ ., data = NHL.regression2) %>%
    step_nzv(all_predictors()) %>% # removes variables with the same value
    step_corr(all_numeric_predictors()) %>%
    step_normalize(all_numeric_predictors()) %>% # important standardization step for LASSO
    step_dummy(all_nominal_predictors()) # creates indicator variables for categorical variables

# model workflow
lm.workflow <- workflow() %>%
    add_recipe(lm.recipe) %>%
    add_model(ols.spec)

# fit the model

full_model <- fit(lm.workflow, data = NHL.regression2) 
full_model %>% tidy()

```

#Calculate and collect CV metrics
```{r}
mod1.cv <- fit_resamples(lm.workflow,
  resamples = NHL.cv6, 
  metrics = metric_set(mae,rsq,rmse)
) %>%

collect_metrics(summarize=TRUE)

mod1.cv

model2.cv<-fit_resamples(lm.workflow, #model refits to different cross validation folds
    resamples=NHL.cv6,metrics = metric_set(mae,rsq,rmse))
model2.cv %>% collect_metrics(summarize=TRUE) #shows rsq, mse, rmse values.
```

#LASSO
```{r}
# Model specifications LASSO
lasso.spec <- 
  linear_reg() %>%
  set_args(mixture = 1, penalty = tune()) %>% ## mixture = 1 indicates Lasso
  set_engine(engine = 'glmnet') %>% #note we are using a different engine
  set_mode('regression') 

# rec is same as OLS

# Workflow (Recipe + Model)
lasso_wf_tune <- workflow() %>% 
  add_recipe(lm.recipe) %>% # recipe defined above
  add_model(lasso.spec) 

# Tune Model (trying a variety of values of Lambda penalty)
penalty_grid <- grid_regular(
  penalty(range = c(0, 8)), #log10 transformed
  levels = 30)


tune_output <- tune_grid( # new function for tuning parameters
  lasso_wf_tune, # workflow
  resamples = NHL.cv6, # cv folds
  metrics = metric_set(rmse, mae),
  grid = penalty_grid # penalty grid defined above
)

# Select best model & fit
best_penalty <- tune_output %>% 
  select_by_one_std_err(metric = 'mae', desc(penalty))

ls_mod <-  best_penalty  %>% 
  finalize_workflow(lasso_wf_tune,.) %>%
  fit(data = NHL.regression2) 
    
# Note which variable is the "least" important    
ls_mod %>% tidy()

Credit_final_wk <- finalize_workflow(lasso_wf_tune, best_penalty) # incorporates penalty value to workflow
 
Credit_final_fit <- fit(Credit_final_wk, data = NHL.regression2)

tidy(Credit_final_fit)

```

#Fit and tune models 
```{r}
tune_output %>% collect_metrics() %>% filter(penalty == (best_penalty %>% pull(penalty)))#metrics for first lasso model 
LASSOCV.cv<-fit_resamples(Credit_final_wk, #model refits to different cross validation folds
    resamples=NHL.cv6)
LASSOCV.cv %>% collect_metrics(summarize=TRUE) #shows rsq, and rmse values.
```

#Visualize redisuals 
```{r}
#Evaluate whether some quantitative predictors might be better modeled with nonlinear relationships

LASSO_mod_output <- NHL.regression2%>%
  bind_cols(predict(Credit_final_fit,new_data=NHL.regression2 ))%>%
  mutate(resid=Salary-.pred)


head(LASSO_mod_output)

ggplot(LASSO_mod_output, aes(x = .pred, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(LASSO_mod_output, aes(x = Ht, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(LASSO_mod_output, aes(x = Wt, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
 
 ggplot(LASSO_mod_output, aes(x = Hand, y = resid)) +
    geom_boxplot() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
 
 ggplot(LASSO_mod_output, aes(x = DftRd, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
 
 ggplot(LASSO_mod_output, aes(x = G, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
 
 ggplot(LASSO_mod_output, aes(x = A1, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
 
ggplot(LASSO_mod_output, aes(x = DftYr, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
 
ggplot(LASSO_mod_output, aes(x = dzFOL, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
 
ggplot(LASSO_mod_output, aes(x = Country, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

 ggplot(LASSO_mod_output, aes(x = GP, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
 
 ggplot(LASSO_mod_output, aes(x = Position, y = resid)) +
    geom_boxplot() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
 
  ggplot(LASSO_mod_output, aes(x = SA, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
 
 ggplot(LASSO_mod_output, aes(x = rookie, y = resid)) +
    geom_boxplot() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
```

Which variables are most important predictors of your quantitative outcome? Justify your answer. Do the methods you've applied reach consensus on which variables are most important? What insights are expected? Surprising? NOTE: if some (but not all) of the indicator terms for a categorical predictor are selected in the final models, the whole predictor should be treated as selected. 


```{r}

glmnet_output <- Credit_final_fit %>% extract_fit_parsnip() %>% pluck('fit') # way to get the original glmnet output

lambdas <- glmnet_output$lambda
coefs_lambdas <- 
  coefficients(glmnet_output, s = lambdas )  %>% 
  as.matrix() %>%  
  t() %>% 
  as.data.frame() %>% 
  mutate(lambda = lambdas ) %>% 
  select(lambda, everything(), -`(Intercept)`) %>% 
  pivot_longer(cols = -lambda, 
               names_to = "term", 
               values_to = "coef") %>%
  mutate(var = map_chr(stringr::str_split(term,"_"),~.[1]))

coefs_lambdas %>%
  ggplot(aes(x = lambda, y = coef, group = term, color = var)) +
  geom_line() +
  geom_vline(xintercept = best_penalty %>% pull(penalty), linetype = 'dashed') + 
  theme_classic() + 
  theme(legend.position = "bottom", legend.text=element_text(size=8))
```


Best overall model based on investigations so far? Predictive accuracy? Interpretability? A combination of both?
```{r}
tune_output %>% collect_metrics() %>% filter(penalty == (best_penalty %>% pull(penalty)))#metrics for first lasso model 
LASSOCV.cv<-fit_resamples(Credit_final_wk, #model refits to different cross validation folds
    resamples=NHL.cv6,metrics = metric_set(mae,rsq,rmse))
LASSOCV.cv %>% collect_metrics(summarize=TRUE) 
mod1.cv <- fit_resamples(lm.workflow,
  resamples = NHL.cv6, 
  metrics = metric_set(mae,rsq,rmse)
) %>%

collect_metrics(summarize=TRUE)

mod1.cv

model2.cv<-fit_resamples(lm.workflow, #model refits to different cross validation folds
    resamples=NHL.cv6,metrics = metric_set(mae,rsq,rmse))
model2.cv %>% collect_metrics(summarize=TRUE) #shows rsq, mse, rmse values.
```

Summarize investigations
Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?
> We are unclear what the best model is based on our investigations thus far. We are aware that a lot of our variables are not linear as shown in our residual plots. We also know that some of our variables willl likely need to be transformed and we will possbily have to include an interaction term in our regression models. 
Our goals include understanding which of these 14 variables predicts the NHL salary of all players. Right now, there is terrible predictive accuracy. 


Are there any harms that may come from your analyses and/or how the data were collected? What cautions do you want to keep in mind when communicating your work? 

> By making these assessments and pushing out our findings we could be harming outlier players. For example, if our models end up showing that athletes of specific height and specific weight are more likely to succeed, incoming athletes into the NHL may start to desire those weights which could harm them psychologically. However, despite being a weight that may get less pay, there is a possibility that they are an outlier player who could get paid more. 

> Additionally, this data is from the 2016 to 2017 season. As the economy changes, inflation occurs, and the interest in the NHL fluctuates, this will influence the salary of players. We want to keep in mind that when we communicate this data, we make it clear the time period this data reflects and make it known that it may not be completely applicable to previous or future NHL season. 


== Homework 2 ==

#Updated for using natural splines
```{r}
# Use natural splines for some of the quantitative predictors to account for non-linearity (GAMs)
# Use OLS engine
# Update recipe to include step_ns() for each quantitative predictor you want to allow to be non-linear
# Determine number of knots (deg_free) and fit a smoothing spline and use edf to inform your choice
```

```{r}
ggplot(NHL.regression2, aes(x=Ht, y=Salary)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    theme_classic()

ggplot(NHL.regression2, aes(x=Wt, y=Salary)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    theme_classic()

ggplot(NHL.regression2, aes(x=Hand, y=Salary)) +
    geom_boxplot() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    theme_classic()

ggplot(NHL.regression2, aes(x=DftRd, y=Salary)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    theme_classic()

ggplot(NHL.regression2, aes(x=G, y=Salary)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    theme_classic()

ggplot(NHL.regression2, aes(x=A1, y=Salary)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    theme_classic()

ggplot(NHL.regression2, aes(x=DftYr, y=Salary)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    theme_classic()

ggplot(NHL.regression2, aes(x=dzFOL, y=Salary)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    theme_classic()
 
 ggplot(NHL.regression2, aes(x=GP, y=Salary)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    theme_classic()
 
 ggplot(NHL.regression2, aes(x=Position, y=Salary)) +
    geom_boxplot() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    
    theme_classic()
 
 ggplot(NHL.regression2, aes(x=SA, y=Salary)) +
    geom_point() +
    geom_smooth(color = "blue", se = FALSE) +
    geom_smooth(method = "lm", color = "red", se = FALSE) +
    theme_classic()
```
```{r}
set.seed(123)
# Model Spec
lm_spec <-
  linear_reg() %>%
  set_engine(engine = 'lm') %>%
  set_mode('regression')

NHL <- drop_na(NHL.regression2)

# Original Recipe
lm_rec <- recipe(Salary ~ Ht + Wt + DftRd + G + A1 + DftYr + dzFOL + GP + SA+ rookie, data = NHL) %>%
  step_naomit(all_numeric(), skip = TRUE) %>%
#  step_ns(Ht, deg_free = 3) %>%
  step_ns(Wt, deg_free = 3) %>%
  step_ns(DftRd, deg_free = 3) %>%
  step_ns(G, deg_free = 3) %>%
#  step_ns(A1, deg_free = 3) %>%
  step_ns(DftYr, deg_free = 3) %>%
#  step_ns(dzFOL, deg_free = 3) %>%
  # step_ns(Country, deg_free = 3) %>%
  step_ns(GP, deg_free = 3) %>%
  step_ns(SA, deg_free = 3)

splines_wf <- workflow() %>% 
  add_recipe(lm_rec) %>%
  add_model(lm_spec)
NHL.cv6 <- vfold_cv(NHL, v=6)

# CV to Evaluate
cv_output <- fit_resamples(
  splines_wf, # workflow
  resamples = NHL.cv6, # cv folds
  metrics = metric_set(mae)
)

# cv_output[[4]][[1]]$.notes

cv_output %>% 
  collect_metrics()

# Fit with all data
ns_mod <- fit(
  splines_wf, #workflow
  data = NHL
)

```

```{r}

spline_mod_output <- NHL %>%
    bind_cols(predict(ns_mod, new_data = NHL)) %>% #generates a column of predictions 
    # bind_cols(NHL) %>% #takes what you are inputting and binds it to something else
    mutate(resid = Salary - .pred)

head(spline_mod_output)

ggplot(spline_mod_output, aes(x = .pred, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()


# Residuals vs. predictors (x's)
#ggplot(spline_mod_output, aes(x = Ht, y = resid)) +
#    geom_point() +
#    geom_smooth() +
#    geom_hline(yintercept = 0, color = "red") +
#    theme_classic()

ggplot(spline_mod_output, aes(x = Wt, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(spline_mod_output, aes(x = DftRd, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(spline_mod_output, aes(x = G, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

#ggplot(spline_mod_output, aes(x = A1, y = resid)) +
#    geom_point() +
#    geom_smooth() +
#    geom_hline(yintercept = 0, color = "red") +
#    theme_classic()

ggplot(spline_mod_output, aes(x = DftYr, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

#ggplot(spline_mod_output, aes(x = dzFOL, y = resid)) +
#    geom_point() +
#    geom_smooth() +
#    geom_hline(yintercept = 0, color = "red") +
#    theme_classic()

ggplot(spline_mod_output, aes(x = GP, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()

ggplot(spline_mod_output, aes(x = SA, y = resid)) +
    geom_point() +
    geom_smooth() +
    geom_hline(yintercept = 0, color = "red") +
    theme_classic()
```

```{r}
tune_output %>% collect_metrics() %>% filter(penalty == (best_penalty %>% pull(penalty)))

cv_output %>% collect_metrics()
```
```{r}
# make a plot of predicted vs actual for the salary 

spline_mod_output %>%
  ggplot()+
  geom_point(aes(x=Salary, y=.pred))+
  theme_minimal()
```



Compare insights from variable importance analyses here and the corresponding results from Homework 1.
Now after having accounted for nonlinearity, have the most relevant predictors changed?
> Looking at our analysis of our linear model, the most relevant predictors are shots on goal allowed while players were on ice (SA), handedness of players (Hand), round in which the player was drafted (DftRd), players’ first assists (A1), and number of goals (G). 
Now that we have accounted for non-linearity, the most relevant predictors appear to be 

Do you gain any insights from the GAM output plots (easily obtained from fitting smoothing splines) for each predictor?
> Salary for hockey players is capped at some point, no matter how many seasonal assists and other ways that they contribute to the team. This relationship is inherently nonlinear, so it is helpful to have a GAM to model that relationship. 
The more goals and individual scores and the more games played and shots against opponent, the higher the residulas become and the more variability in the accuracy of the prediction. 

Compare model performance between your GAM models to the model that assume linearity. 
How does test performance of the GAMs compare to other models you explored?
> Our GAM model has a mean test error of 1,087,752 while the model that assumes linearity has a test error of 125,872. The test performace of the GAMs/splines made the model much worse than the model that assumed linearity. It may also be due to the fact that we took out some variables that we believed were linear variables and perhaps those variables contributed a lot to achieving a low test error. 

Decide on an overall best model based on your investigations so far. To do this, make clear your analysis goals. Predictive accuracy? Interpretability? A combination of both?
> We hope to have predictive accuracy because playing in the NHL is a job and those considering this job may want to understand how their performance on the team will play a role on how much they earn each season. Therefore, we believe that the model that assumes linearity is the best model because it has the lowest test error compared to GAMs by a lot. 

Are there any harms that may come from your analyses and/or how the data were collected? What cautions do you want to keep in mind when communicating your work?
> We hope to change the draft round and edit the data set a little bit. SOMETHING ABOUT THE DRAFT YEAR???? FILL THIS PART IN???



## LOGISTIC REGRESSION 

```{r}
summary(NHL$Hand)
```
```{r}
NHL <- NHL %>%
  mutate(success = as.factor(ifelse(NHL$Salary>2500000,1,0)))%>%
  select(-Salary)

```

```{r}
# Make sure you set reference level (to the outcome you are NOT interested in)
NHL <- NHL %>%
  mutate(NHL = relevel(factor(success), ref='0'))%>% #set reference level
  select(-dzFOL)
# Logistic Regression Model Spec
logistic_spec <- logistic_reg() %>%
    set_engine('glm') %>%
    set_mode('classification')

# Recipe
logistic_rec <- recipe(success ~ Hand + rookie + Position + SA + A1 + G, data = NHL)

# Workflow (Recipe + Model)
log_wf <- workflow() %>% 
    add_recipe(logistic_rec) %>%
    add_model(logistic_spec) 

# Fit Model
log_fit <- fit(log_wf, data = NHL)
```

```{r}
tidy(log_fit)
```

```{r}
# Print out Coefficients
log_fit %>% tidy()

# Get Exponentiated coefficients + CI
log_fit %>% tidy() %>%
  mutate(OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error)) %>% # do this first
  mutate(OR = exp(estimate))
```
```{r}
predict(log_fit, new_data = data.frame(Hand = 'L', rookie = 1, Position= 'D', SA = 100, A1 = 10, G = 20), type = 'prob')
predict(log_fit, new_data = data.frame(Hand = 'L', rookie = 0, Position= 'D', SA = 100, A1 = 10, G = 20), type = 'class')
```
```{r}
# Soft predictions
logistic_output <-  NHL %>%
  bind_cols(predict(log_fit, new_data = NHL, type = 'prob')) 

head(logistic_output)

#logistic_output$pred1 <- logistic_output$.pred_1
#logistic_output$pred0 <- logistic_output$.pred_0
  
logistic_output %>%
  ggplot()+
  geom_boxplot(aes(x=success, y= .pred_1))+
  theme_minimal()+
  geom_hline(yintercept = 0.4, color = 'red', linetype = 2)
```
```{r}
# Hard predictions (you pick threshold)
logistic_output <- logistic_output %>%
  mutate(.pred_1 = probably::make_two_class_pred(.pred_0, levels(success), threshold = 0.4)) 

# Confusion Matrix
logistic_output %>%
  conf_mat(truth = success, estimate = .pred_1)

log_metrics <- metric_set(sens, yardstick::spec, accuracy)
logistic_output %>% 
  log_metrics(estimate = .pred_1, truth = success, event_level = "second")
```

```{r}
#logistic_output$success = as.numeric(logistic_output$success)
#logistic_output$.pred_1 = as.numeric(logistic_output$.pred_1)
logistic_output$preds <- as.numeric(as.factor(logistic_output$.pred_1))

logistic_roc <- logistic_output %>% 
    roc_curve(success, preds, event_level = "second") 

autoplot(logistic_roc) + theme_classic()

logistic_output %>% 
  roc_auc(success, preds, event_level = "second")
```
```{r}
set.seed(123)
data_cv10 <- vfold_cv(NHL, v = 6)

# CV Fit Model
log_cv_fit <- fit_resamples(
    log_wf, 
    resamples = data_cv10,
    metrics = metric_set(sens, yardstick::spec, accuracy, roc_auc),
    control = control_resamples(save_pred = TRUE, event_level = 'second'))  # you need predictions for ROC calculations

collect_metrics(log_cv_fit) #default threshold is 0.5
```

1. Summarize your final model and justify your model choice (see below for ways to justify your choice).
> 

2. Compare the different classification models tried in light of evaluation metrics, variable importance, and data context.
> The logistic regression offers us a number of different ways to evaluate our model for accuracy and usefulness. The logistic model's confusion matrix is really helpful in terms of helping us see when our model is accurately predicting outcomes versus when there are higher rate of false positives or false negatives. Our logistic regression metrics (accuracy, ROC_auc, sensistivity, and specifcity) show us a lot about our logistic model. The sensitivity of our logistic regression is approximately 0.68 (68%) demonstrating that the model predicts true Success  68% of the time. The specificity demosnstrates that our model predicts true Failure values around 87% of the time and our model's overall accuracy of predictions is around 80%.  

Our decision tree model outcomes reflect how successful prediction is the likeliest outcome of the data set when using our chosen variables and metrics given that the final node to the far right is the highest percentage of outcomes. If SA is greater than 488, conditions of success will be met and the prediction will be one of success to our investigation question. 

#Random Forest 

##  Random Forest

### Model Specification

3. Display evaluation metrics for different models in a clean, organized way. This display should include both the estimated metric as well as its standard deviation. (This won’t be available from OOB error estimation. If using OOB, don’t worry about reporting the SD.)
> 

4. Broadly summarize conclusions from looking at these evaluation metrics and their measures of uncertainty.
> Using our evaluation metrics such as the sensitivity and specificity measure of the logistic outcome, we see that our logistic model is a useful one to investigating our research questions and data and our decision tree also provide helpful information when evaluating the salaries of NHL players. 


# - end of logisitic regression - #

```{r}
NHL<-NHL%>%
  select(-NHL)
rf_spec <- rand_forest() %>%
  set_engine(engine = 'ranger') %>% 
  set_args(mtry = NULL, # size of random subset of variables; default is floor(sqrt(ncol(x)))
           trees = 50, # Number of bags
           min_n = 2,
           probability = FALSE, # want hard predictions first
           importance = 'impurity') %>% 
  set_mode('classification') # change this for regression tree

rf_spec
logistic_rec2 <- recipe(success ~ ., data = NHL)

heart_rf_wf <- workflow() %>%
  add_model(rf_spec) %>%
  add_recipe(logistic_rec2)
```



### Fit Models

```{r}
set.seed(123)
heart_rf_fit <- heart_rf_wf %>%
  fit(data = NHL)

heart_rf_fit # check out OOB prediction error (accuracy = 1 - OOB prediction error)
```


### Evaluate Models

To  calculate OOB metrics, we need to get the OOB predictions from the fit model.

```{r}
heart_rf_OOB_output <- tibble(
  .pred_class = heart_rf_fit %>% extract_fit_engine() %>% pluck('predictions'),
  success = NHL %>% pull(success))

bag_metrics <- metric_set(sens, yardstick::spec, accuracy)

heart_rf_OOB_output %>% 
  bag_metrics(truth = success, estimate = .pred_class)
```

To estimate AUC of ROC curve using OOB predictions, we'll need to refit the model to get the predicted probabilities. 

```{r}
set.seed(123) #to get the same bootstrap samples, use same seed
heart_rf_fit2 <- heart_rf_wf %>%
  update_model(rf_spec %>% set_args(probability = TRUE)) %>%
  fit(data = NHL)
head(NHL)
heart_rf_fit2
```

```{r}
heart_rf_OOB_output2 <- bind_cols(
  heart_rf_fit2 %>% extract_fit_engine() %>% pluck('predictions') %>% as_tibble(),
  NHL %>% select(success))

heart_rf_OOB_output2 %>% 
  roc_curve(success, 1, event_level = "first") %>% autoplot()

heart_rf_OOB_output2 %>% 
  roc_auc(success, 1, event_level = "first") #Area under Curve
```

### Variable Importance


```{r}
library(vip) #install.packages('vip')

heart_rf_fit %>% extract_fit_engine() %>% vip() #based on impurity

heart_rf_wf %>% #based on permutation
  update_model(rf_spec %>% set_args(importance = "permutation")) %>%
  fit(data = NHL) %>% extract_fit_engine() %>% vip()
```










