---
title: "Logistic regression"
author: "Claire McHenry"
date: "4/14/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Load in packages 
```{r, message=FALSE, warning=FALSE}
library(dplyr)
library(readr)
library(broom)
library(ggplot2)
library(tidymodels) 
tidymodels_prefer()
library(rpart.plot)
```

#Load in the datasets
```{r}
#Testing data
NHL.test <- read.csv("test.csv")

#Training data
NHL.train <- read.csv("train.csv")
```

#Select 14 variables to use in the regression model
```{r}
#Clean the data so that there are 14 variables we are looking at 
NHL.regression <- NHL.train %>%
  select(Salary, Ht, Wt, Hand, DftRd, G, A1, DftYr, dzFOL, Cntry, GP,Position,SA)

#MGL, OpFOW not found in this dataset 
#MGL = Games lost due to injury 
#OpFOW = Opening faceoffs won
```

#Data cleaning 
```{r}
#Transform the data so that it's as.numeric for Country 
#ideally make the birth year one whole variable instead of a bunch of yes or no (born variables)
NHL.regression2 <- NHL.regression %>%  
  transform(Cntry,Country=as.numeric(factor(Cntry))) %>% mutate(rookie=ifelse(NHL.regression$DftYr>2013,1,0))%>%
  select(Salary, Ht, Wt, Hand, DftRd, G, A1, DftYr, dzFOL, Country, GP, Position, SA,rookie)
NHL.regression2 <- drop_na(NHL.regression2)

NHL <- drop_na(NHL.regression2)
```


## LOGISTIC REGRESSION 

```{r}
summary(NHL$Hand)
```
```{r}
NHL <- NHL %>%
  mutate(success = as.factor(ifelse(NHL$Salary>2500000,1,0)))%>%
  select(-Salary)

```

```{r}
# Make sure you set reference level (to the outcome you are NOT interested in)
NHL <- NHL %>%
  mutate(NHL = relevel(factor(success), ref='0'))%>% #set reference level
  select(-dzFOL)
# Logistic Regression Model Spec
logistic_spec <- logistic_reg() %>%
    set_engine('glm') %>%
    set_mode('classification')

# Recipe
logistic_rec <- recipe(success ~ Hand + rookie + Position + SA + A1 + G, data = NHL)

# Workflow (Recipe + Model)
log_wf <- workflow() %>% 
    add_recipe(logistic_rec) %>%
    add_model(logistic_spec) 

# Fit Model
log_fit <- fit(log_wf, data = NHL)
```

```{r}
tidy(log_fit)
```

```{r}
# Print out Coefficients
log_fit %>% tidy()

# Get Exponentiated coefficients + CI
log_fit %>% tidy() %>%
  mutate(OR.conf.low = exp(estimate - 1.96*std.error), OR.conf.high = exp(estimate + 1.96*std.error)) %>% # do this first
  mutate(OR = exp(estimate))
```
```{r}
predict(log_fit, new_data = data.frame(Hand = 'L', rookie = 1, Position= 'D', SA = 100, A1 = 10, G = 20), type = 'prob')
predict(log_fit, new_data = data.frame(Hand = 'L', rookie = 0, Position= 'D', SA = 100, A1 = 10, G = 20), type = 'class')
```
```{r}
# Soft predictions
logistic_output <-  NHL %>%
  bind_cols(predict(log_fit, new_data = NHL, type = 'prob')) 

head(logistic_output)

#logistic_output$pred1 <- logistic_output$.pred_1
#logistic_output$pred0 <- logistic_output$.pred_0
  
logistic_output %>%
  ggplot()+
  geom_boxplot(aes(x=success, y= .pred_1))+
  theme_minimal()+
  geom_hline(yintercept = 0.4, color = 'red', linetype = 2)
```
```{r}
# Hard predictions (you pick threshold)
logistic_output <- logistic_output %>%
  mutate(.pred_1 = probably::make_two_class_pred(.pred_0, levels(success), threshold = 0.4)) 

# Confusion Matrix
logistic_output %>%
  conf_mat(truth = success, estimate = .pred_1)

log_metrics <- metric_set(sens, yardstick::spec, accuracy)
logistic_output %>% 
  log_metrics(estimate = .pred_1, truth = success, event_level = "second")
```

```{r}
#logistic_output$success = as.numeric(logistic_output$success)
#logistic_output$.pred_1 = as.numeric(logistic_output$.pred_1)
logistic_output$preds <- as.numeric(as.factor(logistic_output$.pred_1))

logistic_roc <- logistic_output %>% 
    roc_curve(success, preds, event_level = "second") 

autoplot(logistic_roc) + theme_classic()

logistic_output %>% 
  roc_auc(success, preds, event_level = "second")
```
```{r}
set.seed(123)
data_cv10 <- vfold_cv(NHL, v = 6)

# CV Fit Model
log_cv_fit <- fit_resamples(
    log_wf, 
    resamples = data_cv10,
    metrics = metric_set(sens, yardstick::spec, accuracy, roc_auc),
    control = control_resamples(save_pred = TRUE, event_level = 'second'))  # you need predictions for ROC calculations

collect_metrics(log_cv_fit) #default threshold is 0.5
```

1. Summarize your final model and justify your model choice (see below for ways to justify your choice).
> 

2. Compare the different classification models tried in light of evaluation metrics, variable importance, and data context.
> The logistic regression offers us a number of different ways to evaluate our model for accuracy and usefulness. The logistic model's confusion matrix is really helpful in terms of helping us see when our model is accurately predicting outcomes versus when there are higher rate of false positives or false negatives. Our logistic regression metrics (accuracy, ROC_auc, sensistivity, and specifcity) show us a lot about our logistic model. The sensitivity of our logistic regression is approximately 0.68 (68%) demonstrating that the model predicts true Success  68% of the time. The specificity demosnstrates that our model predicts true Failure values around 87% of the time and our model's overall accuracy of predictions is around 80%.  

Our decision tree model outcomes reflect how successful prediction is the likeliest outcome of the data set when using our chosen variables and metrics given that the final node to the far right is the highest percentage of outcomes. If SA is greater than 488, conditions of success will be met and the prediction will be one of success to our investigation question. 
